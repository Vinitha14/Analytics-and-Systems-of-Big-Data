{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a context and apply flatMap() to convert each word from the text file to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PySpark',\n",
       " 'is',\n",
       " 'the',\n",
       " 'python',\n",
       " 'binding',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Spark',\n",
       " 'Platform',\n",
       " 'and',\n",
       " 'API',\n",
       " 'and',\n",
       " 'not',\n",
       " 'much',\n",
       " 'different',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Java/Scala',\n",
       " 'versions.',\n",
       " 'A',\n",
       " 'good',\n",
       " 'starting',\n",
       " 'point',\n",
       " 'is',\n",
       " 'the',\n",
       " 'official',\n",
       " 'page',\n",
       " 'i.e',\n",
       " 'Examples',\n",
       " '|',\n",
       " 'Apache',\n",
       " 'Spark.',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'dynamically',\n",
       " 'typed,',\n",
       " 'so',\n",
       " 'RDDs',\n",
       " 'can',\n",
       " 'hold',\n",
       " 'objects',\n",
       " 'of',\n",
       " 'multiple',\n",
       " 'types.',\n",
       " 'PySpark',\n",
       " 'does',\n",
       " 'not',\n",
       " 'yet',\n",
       " 'support',\n",
       " 'a',\n",
       " 'few',\n",
       " 'API',\n",
       " 'calls,',\n",
       " 'such',\n",
       " 'as',\n",
       " 'lookup',\n",
       " 'and',\n",
       " 'non-text',\n",
       " 'input',\n",
       " 'files,',\n",
       " 'though',\n",
       " 'these',\n",
       " 'will',\n",
       " 'be',\n",
       " 'added',\n",
       " 'in',\n",
       " 'future',\n",
       " 'releases.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate();\n",
    "words = sc.textFile(\"wc_input.txt\").flatMap(lambda line: line.split(\" \"))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply map transformation on the words to create new RDDs where each word is given a count of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PySpark', 1),\n",
       " ('is', 1),\n",
       " ('the', 1),\n",
       " ('python', 1),\n",
       " ('binding', 1),\n",
       " ('for', 1),\n",
       " ('the', 1),\n",
       " ('Spark', 1),\n",
       " ('Platform', 1),\n",
       " ('and', 1),\n",
       " ('API', 1),\n",
       " ('and', 1),\n",
       " ('not', 1),\n",
       " ('much', 1),\n",
       " ('different', 1),\n",
       " ('from', 1),\n",
       " ('the', 1),\n",
       " ('Java/Scala', 1),\n",
       " ('versions.', 1),\n",
       " ('A', 1),\n",
       " ('good', 1),\n",
       " ('starting', 1),\n",
       " ('point', 1),\n",
       " ('is', 1),\n",
       " ('the', 1),\n",
       " ('official', 1),\n",
       " ('page', 1),\n",
       " ('i.e', 1),\n",
       " ('Examples', 1),\n",
       " ('|', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark.', 1),\n",
       " ('Python', 1),\n",
       " ('is', 1),\n",
       " ('dynamically', 1),\n",
       " ('typed,', 1),\n",
       " ('so', 1),\n",
       " ('RDDs', 1),\n",
       " ('can', 1),\n",
       " ('hold', 1),\n",
       " ('objects', 1),\n",
       " ('of', 1),\n",
       " ('multiple', 1),\n",
       " ('types.', 1),\n",
       " ('PySpark', 1),\n",
       " ('does', 1),\n",
       " ('not', 1),\n",
       " ('yet', 1),\n",
       " ('support', 1),\n",
       " ('a', 1),\n",
       " ('few', 1),\n",
       " ('API', 1),\n",
       " ('calls,', 1),\n",
       " ('such', 1),\n",
       " ('as', 1),\n",
       " ('lookup', 1),\n",
       " ('and', 1),\n",
       " ('non-text', 1),\n",
       " ('input', 1),\n",
       " ('files,', 1),\n",
       " ('though', 1),\n",
       " ('these', 1),\n",
       " ('will', 1),\n",
       " ('be', 1),\n",
       " ('added', 1),\n",
       " ('in', 1),\n",
       " ('future', 1),\n",
       " ('releases.', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts = words.map(lambda word: (word, 1))\n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey() will perform the merging locally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce. The output will be the total count for each unique word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PySpark', 2),\n",
       " ('is', 3),\n",
       " ('python', 1),\n",
       " ('binding', 1),\n",
       " ('Spark', 1),\n",
       " ('Platform', 1),\n",
       " ('different', 1),\n",
       " ('Java/Scala', 1),\n",
       " ('good', 1),\n",
       " ('starting', 1),\n",
       " ('point', 1),\n",
       " ('official', 1),\n",
       " ('page', 1),\n",
       " ('|', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark.', 1),\n",
       " ('Python', 1),\n",
       " ('dynamically', 1),\n",
       " ('of', 1),\n",
       " ('multiple', 1),\n",
       " ('yet', 1),\n",
       " ('support', 1),\n",
       " ('as', 1),\n",
       " ('input', 1),\n",
       " ('files,', 1),\n",
       " ('though', 1),\n",
       " ('these', 1),\n",
       " ('added', 1),\n",
       " ('in', 1),\n",
       " ('the', 4),\n",
       " ('for', 1),\n",
       " ('and', 3),\n",
       " ('API', 2),\n",
       " ('not', 2),\n",
       " ('much', 1),\n",
       " ('from', 1),\n",
       " ('versions.', 1),\n",
       " ('A', 1),\n",
       " ('i.e', 1),\n",
       " ('Examples', 1),\n",
       " ('typed,', 1),\n",
       " ('so', 1),\n",
       " ('RDDs', 1),\n",
       " ('can', 1),\n",
       " ('hold', 1),\n",
       " ('objects', 1),\n",
       " ('types.', 1),\n",
       " ('does', 1),\n",
       " ('a', 1),\n",
       " ('few', 1),\n",
       " ('calls,', 1),\n",
       " ('such', 1),\n",
       " ('lookup', 1),\n",
       " ('non-text', 1),\n",
       " ('will', 1),\n",
       " ('be', 1),\n",
       " ('future', 1),\n",
       " ('releases.', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts = wordCounts.reduceByKey(lambda a,b:a +b)\n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
