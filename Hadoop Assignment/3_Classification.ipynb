{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.linalg import Vectors \n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we would create a new context but here the entry point to programming Spark with the Dataset and DataFrame API is SparkSession. So, a new session is built and the dataframe is created from the input dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Classifier').getOrCreate()\n",
    "data = spark.read.csv('heart.csv',header = True,inferSchema = True)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the 'target' column to 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, sex: int, cp: int, trestbps: int, chol: int, fbs: int, restecg: int, thalach: int, exang: int, oldpeak: double, slope: int, ca: int, thal: int, label: int]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.withColumnRenamed(\"target\", \"label\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is now a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|label|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|    1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|    1|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|    1|\n",
      "| 56|  1|  1|     120| 236|  0|      1|    178|    0|    0.8|    2|  0|   2|    1|\n",
      "| 57|  0|  0|     120| 354|  0|      1|    163|    1|    0.6|    2|  0|   2|    1|\n",
      "| 57|  1|  0|     140| 192|  0|      1|    148|    0|    0.4|    1|  0|   1|    1|\n",
      "| 56|  0|  1|     140| 294|  0|      0|    153|    0|    1.3|    1|  0|   2|    1|\n",
      "| 44|  1|  1|     120| 263|  0|      1|    173|    0|    0.0|    2|  0|   3|    1|\n",
      "| 52|  1|  2|     172| 199|  1|      1|    162|    0|    0.5|    2|  0|   3|    1|\n",
      "| 57|  1|  2|     150| 168|  0|      1|    174|    0|    1.6|    2|  0|   2|    1|\n",
      "| 54|  1|  0|     140| 239|  0|      1|    160|    0|    1.2|    2|  0|   2|    1|\n",
      "| 48|  0|  2|     130| 275|  0|      1|    139|    0|    0.2|    2|  0|   2|    1|\n",
      "| 49|  1|  1|     130| 266|  0|      1|    171|    0|    0.6|    2|  0|   2|    1|\n",
      "| 64|  1|  3|     110| 211|  0|      0|    144|    1|    1.8|    1|  0|   2|    1|\n",
      "| 58|  0|  3|     150| 283|  1|      0|    162|    0|    1.0|    2|  0|   2|    1|\n",
      "| 50|  0|  2|     120| 219|  0|      1|    158|    0|    1.6|    1|  0|   2|    1|\n",
      "| 58|  0|  2|     120| 340|  0|      1|    172|    0|    0.0|    2|  0|   2|    1|\n",
      "| 66|  0|  3|     150| 226|  0|      1|    114|    0|    2.6|    0|  0|   2|    1|\n",
      "| 43|  1|  0|     150| 247|  0|      1|    171|    0|    1.5|    2|  0|   2|    1|\n",
      "| 69|  0|  3|     140| 239|  0|      1|    151|    0|    1.8|    2|  2|   2|    1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 labels, where 0 implies that a person has heart disease and 1 implies that the person does not have heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    1|\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"label\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be given to a classifer, the attributes need to be concatenated to one column called features. So, for this the label is removed from our features and VectorAssembler() will concatenate the columns. Then the assembler is applied to the original data and transformed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.columns\n",
    "X.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler=VectorAssembler(inputCols = X,outputCol = 'features') \n",
    "out=assembler.transform(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data=out.select('features','label') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the features column has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[63.0,1.0,3.0,145...|    1|\n",
      "|[37.0,1.0,2.0,130...|    1|\n",
      "|[41.0,0.0,1.0,130...|    1|\n",
      "|[56.0,1.0,1.0,120...|    1|\n",
      "|[57.0,0.0,0.0,120...|    1|\n",
      "|[57.0,1.0,0.0,140...|    1|\n",
      "|[56.0,0.0,1.0,140...|    1|\n",
      "|[44.0,1.0,1.0,120...|    1|\n",
      "|[52.0,1.0,2.0,172...|    1|\n",
      "|[57.0,1.0,2.0,150...|    1|\n",
      "|[54.0,1.0,0.0,140...|    1|\n",
      "|[48.0,0.0,2.0,130...|    1|\n",
      "|[49.0,1.0,1.0,130...|    1|\n",
      "|[64.0,1.0,3.0,110...|    1|\n",
      "|[58.0,0.0,3.0,150...|    1|\n",
      "|[50.0,0.0,2.0,120...|    1|\n",
      "|[58.0,0.0,2.0,120...|    1|\n",
      "|[66.0,0.0,3.0,150...|    1|\n",
      "|[43.0,1.0,0.0,150...|    1|\n",
      "|[69.0,0.0,3.0,140...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "proc_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index labels, adding metadata to the label column. Fit on whole dataset to include all labels in index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically identify categorical features, and index them. Set maxCategories so features with > 4 distinct values are treated as continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets (30% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = proc_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a RandomForest model using inbuilt package. Here we're considering number of trees as 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert indexed labels back to original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain indexers and forest in a Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. This also runs the indexers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel (uid=RandomForestClassifier_fb7a9672f022) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "rfModel = model.stages[2]\n",
    "print(rfModel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions by applying the test data to the trained model. Some example rows are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|             1|    1|(13,[0,2,3,4,7,10...|\n",
      "|             1|    1|(13,[0,3,4,7,9,10...|\n",
      "|             1|    1|(13,[0,3,4,7,9,10...|\n",
      "|             1|    1|(13,[0,3,4,7,10,1...|\n",
      "|             1|    1|[29.0,1.0,1.0,130...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MulticlassClassificationEvaluator(), get the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.850575\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
